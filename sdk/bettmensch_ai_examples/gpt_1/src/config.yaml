seed: 10
tokenizer:
  path: tokenizer_train
data:
  train: 
    dataset:
      path: tokenized_data_validation
      context_size: 128
    dataloader:
      batch_size: 45
      shuffle: True
  validation:
    use: True
    dataset:
      path: tokenized_data_validation
      context_size: 128
    dataloader:
      batch_size: 80 # larger than training since no gradients and optimizer state in memory
      shuffle: False
model:
  architecture:
    n_tokens: 128
    dim_embed: 768
    n_decoder_layers: 12
    n_heads: 12
    dropout: 0.1
  misc:
    verbose: False
trainer:
  optimizer:
    lr: 0.00025
    weight_decay: 0.01
    betas:
      - 0.9
      - 0.95
    eps: 0.0001 # 0.0001 needed when running on torch.half dtype
  scheduler:
    linear:
      start_factor: 0.0001
      end_factor: 1
      total_iters: 2000
    cosine:
      T_max: 500
      eta_min: 0
    sequential:
      milestones:
        - 2000
  training:
    grad_clip_norm: 1.0
    n_steps: 1000000 # training duration in batches
    eval_steps: 50 # evaluation interval in batches
    n_steps_eval: 10 # number of batches used for evaluation
    checkpoint_steps: 7500 # checkpointing interval in batches