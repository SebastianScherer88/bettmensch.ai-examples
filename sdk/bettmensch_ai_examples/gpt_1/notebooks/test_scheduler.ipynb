{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import LinearLR, StepLR, MultiStepLR, SequentialLR, CosineAnnealingLR, CosineAnnealingWarmRestarts, ConstantLR\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam([torch.tensor([1])],lr=0.5)\n",
    "linear_scheduler = LinearLR(optimizer, start_factor=0.0001, end_factor=1, total_iters=50)\n",
    "step_scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer,T_max=25,eta_min=0)\n",
    "cosine_restart_scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=25,eta_min=0)\n",
    "sequential_scheduler = SequentialLR(optimizer, schedulers=[linear_scheduler,cosine_scheduler],milestones=[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate for step 0: [0.010049]\n",
      "Learning rate for step 1: [0.020048000000000003]\n",
      "Learning rate for step 2: [0.030047000000000008]\n",
      "Learning rate for step 3: [0.04004600000000001]\n",
      "Learning rate for step 4: [0.05004500000000002]\n",
      "Learning rate for step 5: [0.06004400000000003]\n",
      "Learning rate for step 6: [0.07004300000000002]\n",
      "Learning rate for step 7: [0.08004200000000002]\n",
      "Learning rate for step 8: [0.09004100000000001]\n",
      "Learning rate for step 9: [0.10004]\n",
      "Learning rate for step 10: [0.110039]\n",
      "Learning rate for step 11: [0.12003799999999999]\n",
      "Learning rate for step 12: [0.13003699999999999]\n",
      "Learning rate for step 13: [0.140036]\n",
      "Learning rate for step 14: [0.15003499999999997]\n",
      "Learning rate for step 15: [0.16003399999999995]\n",
      "Learning rate for step 16: [0.17003299999999993]\n",
      "Learning rate for step 17: [0.18003199999999994]\n",
      "Learning rate for step 18: [0.19003099999999995]\n",
      "Learning rate for step 19: [0.20002999999999996]\n",
      "Learning rate for step 20: [0.21002899999999997]\n",
      "Learning rate for step 21: [0.22002799999999997]\n",
      "Learning rate for step 22: [0.23002699999999998]\n",
      "Learning rate for step 23: [0.240026]\n",
      "Learning rate for step 24: [0.25002499999999994]\n",
      "Learning rate for step 25: [0.2600239999999999]\n",
      "Learning rate for step 26: [0.2700229999999999]\n",
      "Learning rate for step 27: [0.28002199999999994]\n",
      "Learning rate for step 28: [0.290021]\n",
      "Learning rate for step 29: [0.30002]\n",
      "Learning rate for step 30: [0.310019]\n",
      "Learning rate for step 31: [0.32001799999999997]\n",
      "Learning rate for step 32: [0.33001699999999995]\n",
      "Learning rate for step 33: [0.340016]\n",
      "Learning rate for step 34: [0.350015]\n",
      "Learning rate for step 35: [0.360014]\n",
      "Learning rate for step 36: [0.370013]\n",
      "Learning rate for step 37: [0.380012]\n",
      "Learning rate for step 38: [0.39001100000000005]\n",
      "Learning rate for step 39: [0.40001000000000003]\n",
      "Learning rate for step 40: [0.41000900000000007]\n",
      "Learning rate for step 41: [0.42000800000000005]\n",
      "Learning rate for step 42: [0.43000700000000003]\n",
      "Learning rate for step 43: [0.440006]\n",
      "Learning rate for step 44: [0.450005]\n",
      "Learning rate for step 45: [0.460004]\n",
      "Learning rate for step 46: [0.47000300000000006]\n",
      "Learning rate for step 47: [0.4800020000000001]\n",
      "Learning rate for step 48: [0.490001]\n",
      "Learning rate for step 49: [0.5]\n",
      "Learning rate for step 50: [0.4980286753286195]\n",
      "Learning rate for step 51: [0.49214579028215777]\n",
      "Learning rate for step 52: [0.48244412147206284]\n",
      "Learning rate for step 53: [0.46907667001096587]\n",
      "Learning rate for step 54: [0.45225424859373686]\n",
      "Learning rate for step 55: [0.4322421568553529]\n",
      "Learning rate for step 56: [0.40935599743717244]\n",
      "Learning rate for step 57: [0.38395669874474914]\n",
      "Learning rate for step 58: [0.35644482289126816]\n",
      "Learning rate for step 59: [0.32725424859373686]\n",
      "Learning rate for step 60: [0.2968453286464312]\n",
      "Learning rate for step 61: [0.26569762988232837]\n",
      "Learning rate for step 62: [0.23430237011767166]\n",
      "Learning rate for step 63: [0.20315467135356885]\n",
      "Learning rate for step 64: [0.17274575140626322]\n",
      "Learning rate for step 65: [0.14355517710873184]\n",
      "Learning rate for step 66: [0.11604330125525078]\n",
      "Learning rate for step 67: [0.09064400256282756]\n",
      "Learning rate for step 68: [0.06775784314464717]\n",
      "Learning rate for step 69: [0.047745751406263165]\n",
      "Learning rate for step 70: [0.030923329989034107]\n",
      "Learning rate for step 71: [0.017555878527937163]\n",
      "Learning rate for step 72: [0.007854209717842259]\n",
      "Learning rate for step 73: [0.0019713246713805588]\n",
      "Learning rate for step 74: [0.0]\n",
      "Learning rate for step 75: [0.001971324671380531]\n",
      "Learning rate for step 76: [0.007854209717842203]\n",
      "Learning rate for step 77: [0.017555878527937135]\n",
      "Learning rate for step 78: [0.030923329989034104]\n",
      "Learning rate for step 79: [0.047745751406263054]\n",
      "Learning rate for step 80: [0.06775784314464703]\n",
      "Learning rate for step 81: [0.09064400256282762]\n",
      "Learning rate for step 82: [0.11604330125525092]\n",
      "Learning rate for step 83: [0.14355517710873195]\n",
      "Learning rate for step 84: [0.1727457514062631]\n",
      "Learning rate for step 85: [0.20315467135356885]\n",
      "Learning rate for step 86: [0.2343023701176717]\n",
      "Learning rate for step 87: [0.2656976298823282]\n",
      "Learning rate for step 88: [0.2968453286464311]\n",
      "Learning rate for step 89: [0.32725424859373675]\n",
      "Learning rate for step 90: [0.3564448228912679]\n",
      "Learning rate for step 91: [0.38395669874474914]\n",
      "Learning rate for step 92: [0.40935599743717227]\n",
      "Learning rate for step 93: [0.4322421568553528]\n",
      "Learning rate for step 94: [0.4522542485937368]\n",
      "Learning rate for step 95: [0.46907667001096576]\n",
      "Learning rate for step 96: [0.4824441214720628]\n",
      "Learning rate for step 97: [0.49214579028215777]\n",
      "Learning rate for step 98: [0.49802867532861944]\n",
      "Learning rate for step 99: [0.49999999999999994]\n"
     ]
    }
   ],
   "source": [
    "scheduler = sequential_scheduler\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    print(f\"Learning rate for step {epoch}: {[pg['lr'] for pg in scheduler.optimizer.param_groups]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_milestones': [50],\n",
       " 'last_epoch': 100,\n",
       " '_last_lr': [0.49999999999999994],\n",
       " '_schedulers': [{'start_factor': 0.0001,\n",
       "   'end_factor': 1,\n",
       "   'total_iters': 50,\n",
       "   'base_lrs': [0.5],\n",
       "   'last_epoch': 49,\n",
       "   'verbose': False,\n",
       "   '_step_count': 50,\n",
       "   '_get_lr_called_within_step': False,\n",
       "   '_last_lr': [0.490001]},\n",
       "  {'T_max': 25,\n",
       "   'eta_min': 0,\n",
       "   'base_lrs': [0.5],\n",
       "   'last_epoch': 50,\n",
       "   'verbose': False,\n",
       "   '_step_count': 52,\n",
       "   '_get_lr_called_within_step': False,\n",
       "   '_last_lr': [0.49999999999999994]}]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
